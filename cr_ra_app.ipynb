{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec54ceb",
   "metadata": {},
   "source": [
    "# ClauseGuard â€” Automated Contract Clause Classification & Risk Advice\n",
    "\n",
    "Automate contract clause understanding end-to-end: classify clauses with fine-tuned LegalBERT and RoBERTa, then generate risk analysis and mitigation guidance with GPT-5 via the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a233b-b7f4-41bc-b281-1e8dcd03d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# !pip install -qU transformers accelerate openai datasets scikit-learn --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f65493-d732-4351-84d2-eb8f4808ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    Trainer, TrainingArguments,\n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    RobertaTokenizerFast, RobertaForSequenceClassification)\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bebbd43a-53f1-4d13-9534-746370930ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU name: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "# --------- Select CPU or GPU to run ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#--------- Check the name of the GPU --------\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169aa7bd-8cef-450c-99b0-b1989c4161ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = \"eb7c6783655ca3a984a33e9a06ad367602057b38\"\n",
    "data_files = {\n",
    "    \"train\": f\"https://huggingface.co/datasets/nguha/legalbench/resolve/{rev}/cuad_audit_rights/train/0000.parquet\",\n",
    "    \"test\":  f\"https://huggingface.co/datasets/nguha/legalbench/resolve/{rev}/cuad_audit_rights/test/0000.parquet\",\n",
    "}\n",
    "dataset = load_dataset(\"parquet\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42e8a63-12fa-4ba9-934c-ca3caa194b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['answer', 'index', 'text', 'document_name'],\n",
       "        num_rows: 6\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['answer', 'index', 'text', 'document_name'],\n",
       "        num_rows: 1216\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61441dc9-ac29-4412-924d-ae5db7fbba6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, Validation, and Test sets size:\n",
      "Train: (977, 4), Validation: (147, 4), Test: (98, 4)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing Data\n",
    "df_train = pd.DataFrame(dataset['test'])\n",
    "df_test  = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Vectorized text cleanup\n",
    "for df in (df_train, df_test):\n",
    "    df['cleaned_text'] = df['text'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Combine, remove any stray 'index' column if present, shuffle, and reset index\n",
    "df_combined = (\n",
    "    pd.concat([df_train, df_test], ignore_index=True)\n",
    "      .drop(columns=['index'], errors='ignore')\n",
    "      .sample(frac=1, random_state=42)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Two-stage split the data into train, val, and test sets\n",
    "train_data, temp = train_test_split(\n",
    "    df_combined,\n",
    "    test_size=0.2,\n",
    "    stratify=df_combined['answer'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_data, test_data = train_test_split(\n",
    "    temp,\n",
    "    test_size=0.4,                  \n",
    "    stratify=temp['answer'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train, Validation, and Test sets size:\")\n",
    "print(f\"Train: {train_data.shape}, Validation: {val_data.shape}, Test: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed481348-1b2f-4767-9e35-4a42913fbec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset class\n",
    "class LegalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e0590d",
   "metadata": {},
   "source": [
    "### Contract Clause Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497329a-2044-4d77-849e-83a0a9247b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluating\n",
    "def train_and_evaluate(X_train, y_train, X_val, y_val, model, tokenizer):\n",
    "\n",
    "    # Tokenize the inputs\n",
    "    train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=512)\n",
    "    val_encodings = tokenizer(X_val.tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Convert labels to tensor\n",
    "    train_labels = torch.tensor(y_train.apply(lambda x: 1 if x.lower() == \"yes\" else 0).tolist())\n",
    "    val_labels = torch.tensor(y_val.apply(lambda x: 1 if x.lower() == \"yes\" else 0).tolist())\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = LegalDataset(train_encodings, train_labels)\n",
    "    val_dataset = LegalDataset(val_encodings, val_labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"epoch\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = val_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    predictions = trainer.predict(val_dataset)\n",
    "    preds = predictions.predictions.argmax(-1)\n",
    "    labels = predictions.label_ids\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4628b3",
   "metadata": {},
   "source": [
    "# Training model with K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aa40581-c812-463a-a1a7-544c56fcaa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv_for_model(model_name: str, save_prefix: str, num_labels: int = 2, n_splits: int = 5):\n",
    "    kf = StratifiedKFold(n_splits=n_splits)\n",
    "    accuracies, precisions, recalls, f1s = [], [], [], []\n",
    "    texts = train_data['cleaned_text']\n",
    "    labels = train_data['answer']\n",
    "    i = 0\n",
    "\n",
    "    for train_index, val_index in kf.split(texts, labels):\n",
    "        X_train = texts[texts.index.isin(train_index)]\n",
    "        X_val   = texts[texts.index.isin(val_index)]\n",
    "        y_train = labels[labels.index.isin(train_index)]\n",
    "        y_val   = labels[labels.index.isin(val_index)]\n",
    "\n",
    "        if model_name==\"roberta-base\":\n",
    "            model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels).to(device)\n",
    "            tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
    "        else:\n",
    "            model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels).to(device)\n",
    "            tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        accuracy, precision, recall, f1 = train_and_evaluate(X_train, y_train, X_val, y_val, model, tokenizer)\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "\n",
    "        model.save_pretrained(f'{save_prefix}-fold{i}')\n",
    "        tokenizer.save_pretrained(f'{save_prefix}-fold{i}')\n",
    "        i += 1\n",
    "\n",
    "    # Print & return averages in the same style\n",
    "    avg_metrics = {\n",
    "        \"model\": model_name,\n",
    "        \"Average Accuracy\": float(np.mean(accuracies)),\n",
    "        \"Average Precision\": float(np.mean(precisions)),\n",
    "        \"Average Recall\": float(np.mean(recalls)),\n",
    "        \"Average F1 Score\": float(np.mean(f1s)),\n",
    "    }\n",
    "    print(f\"[{model_name}] Average Accuracy: {avg_metrics['Average Accuracy']}\")\n",
    "    print(f\"[{model_name}] Average Precision: {avg_metrics['Average Precision']}\")\n",
    "    print(f\"[{model_name}] Average Recall: {avg_metrics['Average Recall']}\")\n",
    "    print(f\"[{model_name}] Average F1 Score: {avg_metrics['Average F1 Score']}\")\n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d590880-79cf-4e91-8647-2acb1b2dc7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 00:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>0.078655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.144100</td>\n",
       "      <td>0.080963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 00:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.259400</td>\n",
       "      <td>0.121921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.072437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.161886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 00:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.383100</td>\n",
       "      <td>0.204118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.133100</td>\n",
       "      <td>0.101963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.054245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 00:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.433400</td>\n",
       "      <td>0.254963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.318600</td>\n",
       "      <td>0.096709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.137122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 00:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.162900</td>\n",
       "      <td>0.112013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.064060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.146437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nlpaueb/legal-bert-base-uncased] Average Accuracy: 0.9859779501837485\n",
      "[nlpaueb/legal-bert-base-uncased] Average Precision: 0.9832891477640843\n",
      "[nlpaueb/legal-bert-base-uncased] Average Recall: 0.9901002125721228\n",
      "[nlpaueb/legal-bert-base-uncased] Average F1 Score: 0.9866158278312586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 00:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.600989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.080695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.094551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 00:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.331200</td>\n",
       "      <td>0.078338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.105769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.123040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 00:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.117100</td>\n",
       "      <td>0.025350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.054464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.537090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 00:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.263611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.154400</td>\n",
       "      <td>0.125381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.155450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 00:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.134100</td>\n",
       "      <td>0.159144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.252274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.261499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[roberta-base] Average Accuracy: 0.9653650310489835\n",
      "[roberta-base] Average Precision: 0.9823001072720174\n",
      "[roberta-base] Average Recall: 0.9498450973589472\n",
      "[roberta-base] Average F1 Score: 0.9651516244333506\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Average Accuracy</th>\n",
       "      <th>Average Precision</th>\n",
       "      <th>Average Recall</th>\n",
       "      <th>Average F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nlpaueb/legal-bert-base-uncased</td>\n",
       "      <td>0.985978</td>\n",
       "      <td>0.983289</td>\n",
       "      <td>0.990100</td>\n",
       "      <td>0.986616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roberta-base</td>\n",
       "      <td>0.965365</td>\n",
       "      <td>0.982300</td>\n",
       "      <td>0.949845</td>\n",
       "      <td>0.965152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             model  Average Accuracy  Average Precision  \\\n",
       "0  nlpaueb/legal-bert-base-uncased          0.985978           0.983289   \n",
       "1                     roberta-base          0.965365           0.982300   \n",
       "\n",
       "   Average Recall  Average F1 Score  \n",
       "0        0.990100          0.986616  \n",
       "1        0.949845          0.965152  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates = [\n",
    "    (\"nlpaueb/legal-bert-base-uncased\", \"fine-tuned-legal-bert\"),\n",
    "    (\"roberta-base\", \"fine-tuned-roberta\")\n",
    "]\n",
    "results = []\n",
    "for model_name, save_prefix in candidates:\n",
    "    metrics = run_cv_for_model(model_name=model_name, save_prefix=save_prefix, num_labels=2, n_splits=5)\n",
    "    results.append(metrics)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results_path = \"./results/cv_comparison.csv\"\n",
    "df_results.to_csv(df_results_path, index=False)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f183f-df37-405d-9f48-44ecb6057d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fine-tuned-legal-bert-fold0 - Accuracy: 0.9897959183673469, Precision: 1.0, Recall: 0.9795918367346939, F1 Score: 0.9896907216494846\n",
      "Model fine-tuned-legal-bert-fold1 - Accuracy: 1.0, Precision: 1.0, Recall: 1.0, F1 Score: 1.0\n",
      "Model fine-tuned-legal-bert-fold2 - Accuracy: 0.9897959183673469, Precision: 1.0, Recall: 0.9795918367346939, F1 Score: 0.9896907216494846\n",
      "Model fine-tuned-legal-bert-fold3 - Accuracy: 0.9693877551020408, Precision: 0.9423076923076923, Recall: 1.0, F1 Score: 0.9702970297029703\n",
      "Model fine-tuned-legal-bert-fold4 - Accuracy: 0.9897959183673469, Precision: 1.0, Recall: 0.9795918367346939, F1 Score: 0.9896907216494846\n",
      "Model fine-tuned-roberta-fold0 - Accuracy: 1.0, Precision: 1.0, Recall: 1.0, F1 Score: 1.0\n",
      "Model fine-tuned-roberta-fold1 - Accuracy: 1.0, Precision: 1.0, Recall: 1.0, F1 Score: 1.0\n",
      "Model fine-tuned-roberta-fold2 - Accuracy: 0.9387755102040817, Precision: 1.0, Recall: 0.8775510204081632, F1 Score: 0.9347826086956522\n",
      "Model fine-tuned-roberta-fold3 - Accuracy: 0.9897959183673469, Precision: 0.98, Recall: 1.0, F1 Score: 0.98989898989899\n",
      "Model fine-tuned-roberta-fold4 - Accuracy: 0.9897959183673469, Precision: 0.98, Recall: 1.0, F1 Score: 0.98989898989899\n",
      "Testing resutls:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Average Accuracy</th>\n",
       "      <th>Average Precision</th>\n",
       "      <th>Average Recall</th>\n",
       "      <th>Average F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nlpaueb/legal-bert-base-uncased</td>\n",
       "      <td>0.987755</td>\n",
       "      <td>0.988462</td>\n",
       "      <td>0.987755</td>\n",
       "      <td>0.987874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roberta-base</td>\n",
       "      <td>0.983673</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>0.975510</td>\n",
       "      <td>0.982916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             model  Average Accuracy  Average Precision  \\\n",
       "0  nlpaueb/legal-bert-base-uncased          0.987755           0.988462   \n",
       "1                     roberta-base          0.983673           0.992000   \n",
       "\n",
       "   Average Recall  Average F1 Score  \n",
       "0        0.987755          0.987874  \n",
       "1        0.975510          0.982916  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the test function\n",
    "def test_model(X_test, y_test, model_name, model_path, tokenizer_path):\n",
    "    # Load the fine-tuned model and tokenizer\n",
    "    if model_name==\"roberta-base\":\n",
    "        model = RobertaForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_path)\n",
    "    else:\n",
    "        model = BertForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "        tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "    # Tokenize the test texts\n",
    "    test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    # Convert labels to tensor\n",
    "    test_labels_tensor = torch.tensor(y_test.apply(lambda x: 1 if x.lower() == \"yes\" else 0).tolist())\n",
    "\n",
    "    # Create a test dataset\n",
    "    test_dataset = LegalDataset(test_encodings, test_labels_tensor)\n",
    "\n",
    "    # Create a DataLoader for the test dataset\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels.extend(batch['labels'].cpu().numpy())\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds.extend(torch.argmax(outputs.logits, dim=-1).cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# test data\n",
    "test_texts = test_data['cleaned_text']\n",
    "test_labels = test_data['answer']\n",
    "\n",
    "results = []\n",
    "for model_name, save_prefix in candidates:\n",
    "    accuracies, precisions, recalls, f1s = [], [], [], []\n",
    "    for i in range(5):\n",
    "        model_path = f'{save_prefix}-fold{i}'\n",
    "        tokenizer_path = f'{save_prefix}-fold{i}'\n",
    "        accuracy, precision, recall, f1 = test_model(test_texts, test_labels, model_name, model_path, tokenizer_path)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "\n",
    "        print(f\"Model {model_path} - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
    "    \n",
    "    avg_metrics = {\n",
    "        \"model\": model_name,\n",
    "        \"Average Accuracy\": float(np.mean(accuracies)),\n",
    "        \"Average Precision\": float(np.mean(precisions)),\n",
    "        \"Average Recall\": float(np.mean(recalls)),\n",
    "        \"Average F1 Score\": float(np.mean(f1s)),\n",
    "    }\n",
    "    results.append(avg_metrics)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results_path = \"./results/test_comparison.csv\"\n",
    "df_results.to_csv(df_results_path, index=False)\n",
    "print(\"Testing resutls:\")\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa219e8d-4ca6-411a-84f5-37c653508476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not an Audit Clause\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = BertForSequenceClassification.from_pretrained(\"fine-tuned-legal-bert-fold1\").to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"fine-tuned-legal-bert-fold1\")\n",
    "\n",
    "# Function for classification using Legal-BERT\n",
    "def classify_clause(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    preds = torch.argmax(outputs.logits, dim=-1)\n",
    "    return preds.item()\n",
    "\n",
    "# Define a test clause\n",
    "test_clause = 'Neither party shall voluntarily or by operation of law assign or otherwise transfer the rights and/or obligations incurred pursuant to the terms of this Agreement without the prior written consent of the other party.'\n",
    "\n",
    "# Get the combined result\n",
    "response = \"Audit clause\" if classify_clause(test_clause) else \"Not an Audit Clause\"\n",
    "\n",
    "# Print the combined result\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdfe744",
   "metadata": {},
   "source": [
    "# Risk Analysis and Mitigation Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca335eb4-745e-43ec-8e32-851449ff5985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6276b13-4afd-4a9f-b6d9-173b80197f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause: Neither party shall voluntarily or by operation of law assign or otherwise transfer the rights and/or obligations incurred pursuant to the terms of this Agreement without the prior written consent of the other party.\n",
      "\n",
      "Classification: Not an Audit Clause\n",
      "\n",
      "Key risks:\n",
      "- Blocks important corporate changes (mergers, acquisitions, reorganizations) that could affect business continuity or value.\n",
      "- Ambiguity over what constitutes a transfer or assignment, and whether ancillary actions (delegation, novation) trigger the clause.\n",
      "- No standard requiring consent to be reasonably withheld, leading to potential arbitrary or strategic withholding.\n",
      "- No carve-outs for permitted transfers (to affiliates, successors, lenders, or in connection with a sale of all or substantially all assets), increasing disruption risk.\n",
      "- No defined remedies or consequences for unauthorized assignment, creating potential enforcement disputes.\n",
      "- Potential negative impact on financing arrangements or strategic partnerships requiring assignment or collateral rights.\n",
      "- Unclear treatment in insolvency or change-of-control scenarios, risking unintended breach or termination triggers.\n",
      "\n",
      "Mitigations:\n",
      "- Amend to require consent not to be unreasonably withheld, conditioned, or delayed, with a defined response window (e.g., 15â€“30 days).\n",
      "- Add explicit permitted transfers and carve-outs (e.g., to affiliates, to successors in a merger or sale of substantially all assets, to lenders or as security, to service providers under obligation).\n",
      "- Include a change-of-control clause (notice of change, and either termination rights or deemed consent unless objected within a set period).\n",
      "- Define scope of \"transfer\" and \"rights and/or obligations\" to include assignments, novations, and delegations, with examples.\n",
      "- Establish a clear consent process (timelines, deemed consent if no response, and a rationale requirement for withholding).\n",
      "- Specify remedies for unauthorized assignment (void, voidable, or breach-based remedies; potential termination rights).\n"
     ]
    }
   ],
   "source": [
    "# Function to run the prompt using the ChatCompletion endpoint\n",
    "def llm_clause_analysis(classification_label, clause):\n",
    "    message = (\n",
    "        f\"Below is a contract clause classified as '{classification_label}':\\n\\n\"\n",
    "        f\"'{clause}'\\n\\n\"\n",
    "    )\n",
    "    system_prompt = \"\"\"You are a legal advisor. Provide a concise, cohesive explanation linking the clause, its classification, and the listed risks. Use the exact template below:\n",
    "        Clause: <clause>\n",
    "        Classification: <label>\n",
    "        Key risks: <In 5â€“8 bullet points max, flag any risks in the clauses>\n",
    "        Mitigations: <Solutions for risks>\"\"\"\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        instructions=system_prompt,\n",
    "        input=message,\n",
    "    )\n",
    "    return response.output_text\n",
    "\n",
    "# Define a test clause\n",
    "test_clause = \"Neither party shall voluntarily or by operation of law assign or otherwise transfer the rights and/or obligations incurred pursuant to the terms of this Agreement without the prior written consent of the other party.\"\n",
    "\n",
    "classification_result = classify_clause(test_clause)\n",
    "classification_label = \"Audit Clause\" if classification_result else \"Not an Audit Clause\"\n",
    "\n",
    "llm_response = llm_clause_analysis(classification_label, test_clause)\n",
    "\n",
    "# Print the combined result\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41197b08-70cf-4b99-9308-1137e13dcf13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
